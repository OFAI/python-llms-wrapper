{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "744579c6-ec0c-4c73-bb38-5c99a566056f",
   "metadata": {},
   "source": [
    "# dev-tooling.ipynb\n",
    "\n",
    "Develop the API code to support tooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75314c45",
   "metadata": {},
   "source": [
    "## LiteLLM Original Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c48f923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.66.0\n",
      "\n",
      "First LLM Response:\n",
      " ModelResponse(id='chatcmpl-BPA3wPIN6GvkAkMIZMHu0EOoki25d', created=1745336580, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_a6889ffe71', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"San Francisco, CA\"}', name='get_current_weather'), id='call_9AjtVQmGvUqZe6YoChInWxuy', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Tokyo, Japan\"}', name='get_current_weather'), id='call_q1li00gf6ci6ErIE5B2lCuli', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"location\": \"Paris, France\"}', name='get_current_weather'), id='call_C2aHqW8JiT6wBwkkELN3Rb3N', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], usage=Usage(completion_tokens=69, prompt_tokens=85, total_tokens=154, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n",
      "\n",
      "Length of tool calls 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johann/software/anaconda/envs/llms_wrapper/lib/python3.11/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `ChatCompletionMessageToolCall` - serialized value may not be as expected [input_value={'function': {'arguments'...uy', 'type': 'function'}, input_type=dict])\n",
      "  PydanticSerializationUnexpectedValue(Expected `ChatCompletionMessageToolCall` - serialized value may not be as expected [input_value={'function': {'arguments'...li', 'type': 'function'}, input_type=dict])\n",
      "  PydanticSerializationUnexpectedValue(Expected `ChatCompletionMessageToolCall` - serialized value may not be as expected [input_value={'function': {'arguments'...3N', 'type': 'function'}, input_type=dict])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Second LLM response:\n",
      " ModelResponse(id='chatcmpl-BPA3xcUSmxyJ6oZgwvu6nhVHRQOjm', created=1745336581, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_90122d973c', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Here's the current weather in the requested cities:\\n\\n- **San Francisco, CA**: 72°F\\n- **Tokyo, Japan**: 10°C\\n- **Paris, France**: 22°C\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], usage=Usage(completion_tokens=43, prompt_tokens=158, total_tokens=201, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-BPA3xcUSmxyJ6oZgwvu6nhVHRQOjm', created=1745336581, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_90122d973c', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Here's the current weather in the requested cities:\\n\\n- **San Francisco, CA**: 72°F\\n- **Tokyo, Japan**: 10°C\\n- **Paris, France**: 22°C\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]))], usage=Usage(completion_tokens=43, prompt_tokens=158, total_tokens=201, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import litellm\n",
    "import importlib\n",
    "print(importlib.metadata.version(\"litellm\"))\n",
    "import json\n",
    "# set openai api key\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = os.environ['MY_OPENAI_API_KEY']\n",
    "MODEL = \"openai/gpt-4o\"\n",
    "\n",
    "# Example dummy function hard coded to return the same weather\n",
    "# In production, this could be your backend API or an external API\n",
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Tokyo\", \"temperature\": \"10\", \"unit\": \"celsius\"})\n",
    "    elif \"san francisco\" in location.lower():\n",
    "        return json.dumps({\"location\": \"San Francisco\", \"temperature\": \"72\", \"unit\": \"fahrenheit\"})\n",
    "    elif \"paris\" in location.lower():\n",
    "        return json.dumps({\"location\": \"Paris\", \"temperature\": \"22\", \"unit\": \"celsius\"})\n",
    "    else:\n",
    "        return json.dumps({\"location\": location, \"temperature\": \"unknown\"})\n",
    "\n",
    "\n",
    "def test_parallel_function_call():\n",
    "    try:\n",
    "        # Step 1: send the conversation and available functions to the model\n",
    "        messages = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco, Tokyo, and Paris?\"}]\n",
    "        tools = [\n",
    "            {\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": \"get_current_weather\",\n",
    "                    \"description\": \"Get the current weather in a given location\",\n",
    "                    \"parameters\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"properties\": {\n",
    "                            \"location\": {\n",
    "                                \"type\": \"string\",\n",
    "                                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                            },\n",
    "                            \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                        },\n",
    "                        \"required\": [\"location\"],\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        ]\n",
    "        response = litellm.completion(\n",
    "            model=MODEL,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            tool_choice=\"auto\",  # auto is default, but we'll be explicit\n",
    "        )\n",
    "        print(\"\\nFirst LLM Response:\\n\", response)\n",
    "        response_message = response.choices[0].message\n",
    "        tool_calls = response_message.tool_calls\n",
    "\n",
    "        print(\"\\nLength of tool calls\", len(tool_calls))\n",
    "\n",
    "        # Step 2: check if the model wanted to call a function\n",
    "        if tool_calls:\n",
    "            # Step 3: call the function\n",
    "            # Note: the JSON response may not always be valid; be sure to handle errors\n",
    "            available_functions = {\n",
    "                \"get_current_weather\": get_current_weather,\n",
    "            }  # only one function in this example, but you can have multiple\n",
    "            messages.append(response_message)  # extend conversation with assistant's reply\n",
    "\n",
    "            # Step 4: send the info for each function call and function response to the model\n",
    "            for tool_call in tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                function_to_call = available_functions[function_name]\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                function_response = function_to_call(\n",
    "                    location=function_args.get(\"location\"),\n",
    "                    unit=function_args.get(\"unit\"),\n",
    "                )\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"role\": \"tool\",\n",
    "                        \"name\": function_name,\n",
    "                        \"content\": function_response,\n",
    "                    }\n",
    "                )  # extend conversation with function response\n",
    "            second_response = litellm.completion(\n",
    "                model=MODEL,\n",
    "                messages=messages,\n",
    "            )  # get a new response from the model where it can see the function response\n",
    "            print(\"\\nSecond LLM response:\\n\", second_response)\n",
    "            return second_response\n",
    "    except Exception as e:\n",
    "      print(f\"Error occurred: {e}\")\n",
    "\n",
    "test_ret1 = test_parallel_function_call()\n",
    "test_ret1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb18b2",
   "metadata": {},
   "source": [
    "## Own Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6da6e33-e3dd-45d3-abad-ad48a617b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from typing import Optional, List, Dict\n",
    "sys.path.append(os.path.join(\"..\"))\n",
    "from llms_wrapper.llms import LLMS, toolnames2funcs, get_func_by_name\n",
    "from llms_wrapper.config import update_llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7401b8b9-af81-4a1e-9bfa-bef00ec3ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    llms=[\n",
    "        # OpenAI\n",
    "        # https://platform.openai.com/docs/models\n",
    "        dict(llm=\"openai/gpt-4o\"),\n",
    "        dict(llm=\"openai/gpt-4o-mini\"),\n",
    "        # dict(llm=\"openai/o1\"),        # restricted\n",
    "        # dict(llm=\"openai/o1-mini\"),   # restricted\n",
    "        # Google Gemini\n",
    "        # https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "        dict(llm=\"gemini/gemini-2.0-flash-exp\"),\n",
    "        dict(llm=\"gemini/gemini-1.5-flash\"),\n",
    "        dict(llm=\"gemini/gemini-1.5-pro\"),\n",
    "        # Anthropic\n",
    "        # https://docs.anthropic.com/en/docs/about-claude/models\n",
    "        dict(llm=\"anthropic/claude-3-5-sonnet-20240620\"),\n",
    "        dict(llm=\"anthropic/claude-3-opus-20240229\"),\n",
    "        # Mistral\n",
    "        # https://docs.mistral.ai/getting-started/models/models_overview/\n",
    "        dict(llm=\"mistral/mistral-large-latest\"),\n",
    "        # XAI\n",
    "        # dict(llm=\"xai/grok-2\"),     # not mapped by litellm yet?\n",
    "        dict(llm=\"xai/grok-beta\"),\n",
    "        # Groq\n",
    "        # https://console.groq.com/docs/models\n",
    "        dict(llm=\"groq/llama3-70b-8192\"),\n",
    "        dict(llm=\"groq/llama-3.3-70b-versatile\"),\n",
    "        # Deepseek\n",
    "        # https://api-docs.deepseek.com/quick_start/pricing\n",
    "        dict(llm=\"deepseek/deepseek-chat\"),\n",
    "    ],\n",
    "    providers = dict(\n",
    "        openai = dict(api_key_env=\"MY_OPENAI_API_KEY\"),\n",
    "        gemini = dict(api_key_env=\"MY_GEMINI_API_KEY\"),\n",
    "        anthropic = dict(api_key_env=\"MY_ANTHROPIC_API_KEY\"),\n",
    "        mistral = dict(api_key_env=\"MY_MISTRAL_API_KEY\"),\n",
    "        xai = dict(api_key_env=\"MY_XAI_API_KEY\"),    \n",
    "        groq = dict(api_key_env=\"MY_GROQ_API_KEY\"),\n",
    "        deepseek = dict(api_key_env=\"MY_DEEPSEEK_API_KEY\"),\n",
    "    )\n",
    ")\n",
    "config = update_llm_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "078083b6-dcf4-4d7e-9950-36ffb058732a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = LLMS(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a2eb12c-7219-49ee-8f47-e3e87b719795",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['openai/gpt-4o',\n",
       " 'openai/gpt-4o-mini',\n",
       " 'gemini/gemini-2.0-flash-exp',\n",
       " 'gemini/gemini-1.5-flash',\n",
       " 'gemini/gemini-1.5-pro',\n",
       " 'anthropic/claude-3-5-sonnet-20240620',\n",
       " 'anthropic/claude-3-opus-20240229',\n",
       " 'mistral/mistral-large-latest',\n",
       " 'xai/grok-beta',\n",
       " 'groq/llama3-70b-8192',\n",
       " 'groq/llama-3.3-70b-versatile',\n",
       " 'deepseek/deepseek-chat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llms.list_aliases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c7f6986-347b-4e50-94bb-31272e798130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_names(where_clause: str) -> List[str]: \n",
    "    \"\"\"\n",
    "    Query the customer database and return a list of matching names. \n",
    "\n",
    "    This function queries the customer database using the conditions in the where clause and returns\n",
    "    a list of matching customers. The where clause may use the DB fields \"city\", \"company_name\",\n",
    "    \"country\" and \"since_date\" to limit the returned customer list. The where clause can also \n",
    "    be followed by a limit clause to limit the number of returned names. \n",
    "\n",
    "    :param where_clause: the string containing the where and optionally limit clauses in SQL query format\n",
    "    :type where_clause: string\n",
    "    :return: a list of matching customer names\n",
    "    :rtype: array\n",
    "    \"\"\"\n",
    "    return [\"Monica Schmidt\", \"Harald Mueller\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0cbe8851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found query_names as a function\n"
     ]
    }
   ],
   "source": [
    "tmpfunc = get_func_by_name(\"query_names\")\n",
    "if callable(tmpfunc):\n",
    "    print(f\"Found {tmpfunc.__name__} as a function\")\n",
    "else:\n",
    "    print(f\"Found {tmpfunc} but not a function, type {type(tmpfunc)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4722fb91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Monica Schmidt', 'Harald Mueller']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmpfunc(\"city='Berlin' and company_name='Acme Corp' and since_date='2023-01-01' and limit=10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b49ca6c6-e5ef-4a92-81b2-307c4c01143a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'query_names',\n",
       "   'description': 'Query the customer database and return a list of matching names. \\n\\nThis function queries the customer database using the conditions in the where clause and returns\\na list of matching customers. The where clause may use the DB fields \"city\", \"company_name\",\\n\"country\" and \"since_date\" to limit the returned customer list. The where clause can also \\nbe followed by a limit clause to limit the number of returned names.',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'where_clause': {'type': 'string',\n",
       "      'description': 'the string containing the where and optionally limit clauses in SQL query format'}},\n",
       "    'required': ['where_clause']}}}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools=llms.make_tooling(query_names)\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2b274f9-a160-4074-a022-3d377fdc82fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llms.supports_function_calling(\"openai/gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07928eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llms.supports_function_calling(\"openai/gpt-4o\", parallel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "930d815e-d4fc-4a2e-aba0-4d4593900f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'Give me the names of customers in New York which have been customers since 2023 or longer',\n",
       "  'role': 'user'}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = llms.make_messages(\"Give me the names of customers in New York which have been customers since 2023 or longer\")\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "338e3b72-2ccc-47a0-9248-c422a232fdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johann/software/anaconda/envs/llms_wrapper/lib/python3.11/site-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
      "  PydanticSerializationUnexpectedValue(Expected `ChatCompletionMessageToolCall` - serialized value may not be as expected [input_value={'function': {'arguments'...xq', 'type': 'function'}, input_type=dict])\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('The customers in New York who have been customers since 2023 or longer are:\\n\\n- Monica Schmidt\\n- Harald Mueller',\n",
       " '')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = llms.query(\"openai/gpt-4o\", messages=msgs, tools=tools, return_cost=True)\n",
    "ret[\"answer\"], ret[\"error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1fd40ed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elapsed_time': 1.8553009033203125,\n",
       " 'cost': 0.0015425,\n",
       " 'n_completion_tokens': 63,\n",
       " 'n_prompt_tokens': 365,\n",
       " 'n_total_tokens': 428,\n",
       " 'finish_reason': 'stop',\n",
       " 'answer': 'Here are the names of customers in New York who have been customers since 2023 or longer:\\n\\n1. Monica Schmidt\\n2. Harald Mueller',\n",
       " 'error': '',\n",
       " 'ok': True}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edd371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b61f1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeec211c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms_wrapper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
